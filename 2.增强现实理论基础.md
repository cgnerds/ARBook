# 增强现实理论基础

## 理论基础简介

本章将对增强现实技术中所涉及的计算机视觉和计算机图形学基础知识、摄像机对现实场景的理解和一些经典计算机视觉算法进行详细介绍，包括目标的检测与识别以及对目标的跟踪定位等。

目标的检测与识别的目的是发现并找到场景中的目标，并通过计算确定它在真实场景中的位置，进而才能够与渲染的虚拟影像相叠加。在增强现实中，目标一般指代的是一副图像或一个立体物体，通过目标检测与识别可以实现标记式的增强现实。目前，检测和识别技术的难点在于目标的碎片化，对立体物体来说，每一件立体物体都有其独特的特征，而不同特征的提取和处理都要实现一一对应，这对检测和识别是巨大的挑战。就平面图像而言，平面图像本身还受到噪声、尺度、旋转和光照等因素的影响，在不同的环境下特征的表现可能各不相同。本章将会介绍几种目标检测和识别的经典算法，它们都具有一定的实用性。

对目标的跟踪定位算法一般采用实时检测与先验知识优化优化相结合的方式解决跟踪精度的问题。在小范围的特定场景中，模板匹配是一种速度快、数据量小且系统简单的跟踪定位算法，但是图像本身的噪声、尺度、旋转和光照等变化会对跟踪精读造成较大影响。在大场景下，一般使用SLAM算法进行处理。其优点是不需要预存场景，可以跟踪较大范围，使用面广，在跟踪的同时可以完成对场景结构的重建。但目前这类技术计算速度慢、数据量大、算法复杂度高、对系统的要求也较高。在手机上普及还需要对算法做一些精简，但同时会损失精度。

## 摄像机空间理论

### 摄像机透视模型

### 摄像机参数矩阵

## 图像局部特征的提取与匹配

### SIFT特征描述子

### ORB特征描述子

### 模板匹配

### 位姿估计

### 用于运动状态预测的滤波器

### 即时定位于地图构建系统



## 计算机图形学基础

### 齐次坐标系[^1][^2]

#### 问题：两条平行线可以相交于一点 

在欧氏空间（几何），同一平面的两条平行线不能相交，或永远不能相遇。这是我们都熟悉的常识。

然而，在透视空间里面，两条平行线可以相交，例如：在下图中，火车轨道离我们的视线越远，变得越窄。最后，两条平行线相交于地平线处，即无穷远处的一点。

![railroad.jpg](imgs/5b716bf2d6d8a.jpg)

欧氏空间（或笛卡尔空间）描述 2D / 3D 几何非常适合，但是这种方法却不适合处理透视空间的问题（实际上，欧氏几何是透视几何的一个子集合），2D 点的笛卡尔坐标可以表示为$\left( x,y \right)$ 。

如果一个点在无穷远处，这个点的坐标将会是$\left( \infty ,\infty \right) $，在欧氏空间，这变得没有意义。平行线在透视空间的无穷远处交于一点，但是在欧氏空间却不能。数学家发现了一种方法来解决这个问题。  

#### 方法：齐次坐标 

August Ferdinand Möbius 引入了齐次坐标，使得在透视空间进行图形和几何计算成为可能。齐次坐标是一种使用N+1个数来代表N维坐标的方法。

为了构建 2D 齐次坐标，我们可以在现有笛卡尔坐标基础上，加一个额外的变量 $w$ 。因此，笛卡尔坐标系内的点$\left( X,\,\,Y \right) $ 在齐次坐标系里面变成了 $\left( x,\,\,y,\,\,z \right) $ ，并且有
$$
\begin{cases}
	X=x\,\,/\,\,w\\
	Y=y\,\,/\,\,w\\
\end{cases}
$$
比如，笛卡尔坐标系下的点（1，2）的齐次坐标可以表示为（1，2，1）。如果点（1，2）移动到无限远处，在笛卡尔坐标下它变为 $\left( \infty ,\infty \right) $ ，然后它的齐次坐标表示为（1，2，0），因为 $\left( \text{1/0,2/}0 \right) \approx \left( \infty ,\,\,\infty \right) $ 。注意，我们可以不用 “ $\infty$ “ 来表示一个无穷远处的点了。

#### 为什么叫齐次坐标？ 

如上所述，为了将齐次坐标  $\left( x,\,\,y,\,\,z \right) $ 转化为笛卡尔坐标$\left( X,\,\,Y \right) $ ，只需将 $x$ 和 $y$ 除以 $w$ 即可：
$$
\begin{array}{c}
	\left( x,y,w \right)\\
	Homogeneous\\
\end{array}\Leftrightarrow \begin{array}{c}
	\left( \frac{x}{w},\frac{y}{w} \right)\\
	Cartesian\\
\end{array}
$$
转化齐次坐标到笛卡尔坐标的过程中，我们有一个重要发现，请看下面的例子： 
$$
\begin{matrix}
	Homogeneous&		Cartesian\\
\end{matrix}
\\
\left( \text{1,2,}3 \right) \Longrightarrow \left( \frac{1}{3},\frac{2}{3} \right) 
\\
\left( \text{2,4,}6 \right) \Longrightarrow \left( \frac{2}{6},\frac{4}{6} \right) =\left( \frac{1}{3},\frac{2}{3} \right) 
\\
\left( \text{4,8,}12 \right) \Longrightarrow \left( \frac{4}{12},\frac{8}{12} \right) =\left( \frac{1}{3},\frac{2}{3} \right) 
\\
\vdots \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\vdots \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
\\
\left( 1a,2a,3a \right) \Longrightarrow \left( \frac{1a}{3a},\frac{2a}{3a} \right) =\left( \frac{1}{3},\frac{2}{3} \right)
$$
你会发现点 (1, 2, 3) , (2, 4, 6) 和 (4, 8, 12) 对应同一个欧氏空间点 (1/3, 2/3)。在欧氏空间中，任何标量乘积，如(1a, 2a, 3a) 对应同一个点 (1/3, 2/3) 。因此，这些点是“齐次的” （译者注：含有“同类”的意思），因为他们表示欧氏空间（或笛卡尔空间）里面的同一个点。换句话说，齐次坐标有尺度不变性。 

#### 证明：两条直线可以相交

考虑欧氏空间中的线性方程组： 

$$
\begin{cases}
	Ax+By+C=0\\
	Ax+By+D=0\\
\end{cases}
$$

我们知道如果C ≠ D，上述方程组无解。如果C=D，两条直线为同一条直线（重叠）。我们在透视空间重写上述方程式，用齐次坐标 $x/w$, $y/w$ 代替$x$ , $y$, 
$$
\begin{cases}
	A\frac{x}{w}+B\frac{y}{w}+C=0\\
	A\frac{x}{w}+B\frac{y}{w}+D=0\\
\end{cases}\Longrightarrow \begin{cases}
	Ax+By+Cw=0\\
	Ax+By+Dw=0\\
\end{cases}
$$
现在我们有一个解$\left( x,y,0 \right) $，因为 $\left( C-D \right) w=0$ ，所以 $w=0$ 。因此，两条直线相交于$\left( x,y,0 \right) $，这个点在无穷远处。

在诸如将 3D 场景投影到 2D 平面的过程中，齐次坐标是一个非常有用和基础的计算机图形学概念。

------

[^1]: http://www.songho.ca/math/homogeneous/homogeneous.html, Homogeneous Coordinates
[^2]: https://blog.csdn.net/janestar/article/details/44244849, 关于齐次坐标的理解（经典）

### 场景形状在增强现实和虚拟现实的应用

获取场景形状可应用于增强现实和混合现实的很多方面。这里介绍相关的若干应用案例。

#### 位姿推定

PTAM、DTAM等常用的跟踪方法是测量摄像机相对于局部坐标系的位姿的方法。而为了在空间绝对位置（全局坐标系）上表示虚拟物体，事先需要建立现实世界的三维位置数据库。场景形状模型、SfM等确定的特征点三维位置信息用于该数据库的建立。另外，如果输入是二维影像，那么，用于测量位姿的数据库内容通常由特征点三维位置和图像特征量构成。图像特征量可根据SIFT、SURF等方法算出，但这些方法容易受光源变化的影响。为此，有的方法提出，利用形状模型和色彩模型重构针对当前光源环境的现实世界外观，再通过由重构得到的图像和输入图像之间的匹配，准确地测量出位姿，适应光源环境的外观重构与匹配。

#### 遮挡处理

遮挡问题实际上就是因场景深度信息无法获得，从而导致叠加显示的虚拟模型和场景深度不能按照正确的顺序呈现出来。遮挡给人的深度知觉产生较大影响，是实现无不协调感的增强现实和混合现实必须呈现的重要元素之一。RGBD 摄像机能够对输入图像的每个像素实时获取深度信息，所以依照一般的深度判定便可以表现遮挡，但是，如果遇到室外场景无法实时获得深度信息的话，可以利用事先获得的场景形状进行位置配准，同时从场景形状中获得深度信息，从而正确地表现。

#### 重光照

给场景加入虚拟物体和虚拟光照，要求同时改变现实物体的阴影和色彩。为了确定现实物体的外观，需要利用场景的形状和色彩模型对外观的变化进行模拟仿真。建立色彩模型的方法有很多，但输入二维图像时，需要同时计算形状、色彩和光源，是个不适定问题。此时，如果场景形状已知的话，便可以作为重要的线索信息，便用事先已求得的场景形状，或者便用RGBD摄像机获取的深度图像。另外，空间型增强现实（SAR），为了将虚拟物体投影到现实世界中且毫无不协调感，必须进行利用形状模型的几何和光学补偿修正。

从以上示例可知，为了在增强现实和混合现实中正确地表现虚拟世界和现实世界的一致性，场景的形状模型具有重要作用。近年来，除了高价的激光距离传感器外，Kinect等低廉的RGBD摄像机已经开始得到普及，所以，形状模型化技术将会成为更广泛应用的技术。

### Unity RenderQueue

| Render queue |                   Render queue description                   | Render queue value |
| ------------ | :----------------------------------------------------------: | ------------------ |
| Background   | This render queue is rendered first. It is used for skyboxes and so on. | 1000               |
| Geometry     | This is the default render queue. This is used for most objects. Opaque geometry uses this queue. | 2000               |
| AlphaTest    | Alpha-tested geometry uses this queue. It’s different from the Geometry queue as it’s more efficient to render alpha-tested objects after all the solid objects are drawn. | 2450               |
| Transparent  | This render queue is rendered after Geometry and AlphaTest queues in back-to-front order. Anything alpha-blended(that is, shaders that don’t write to the depth buffer) should go here, for example, glass and particle effects. | 3000               |
| Overlay      | This render queue is meant for overlay effects. Anything rendered last should go here, for example, lens flares. | 4000               |

